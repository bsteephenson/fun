{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x10c5da518>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x10c53fb38>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x104e791d0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "session = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# shape = [a, b] means that we have a data points and each is b-dimensional\n",
    "# we write None here to allow for an arbitrary number of data points\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "\n",
    "# this is 10 dimensional because the ith dimension is the probability of the ith digit\n",
    "# since y_ is the actual value, it'll look something like [0, ... 0, 1, 0, ... 0 ]\n",
    "# AKA one shot vectors representing the class of the data point\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# A matrix (linear transformation) that takes points from 784 dim land to 10 dim land\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "# biases because our data isn't centered\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# I don't understand why the order of the multiple is this instead of matmul(W, x)\n",
    "y = tf.matmul(x, W) + b\n",
    "\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Our error function\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = y))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.38039219,  0.37647063,  0.3019608 ,\n",
       "          0.46274513,  0.2392157 ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.35294119,  0.5411765 ,  0.92156869,\n",
       "          0.92156869,  0.92156869,  0.92156869,  0.92156869,  0.92156869,\n",
       "          0.98431379,  0.98431379,  0.97254908,  0.99607849,  0.96078438,\n",
       "          0.92156869,  0.74509805,  0.08235294,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.54901963,\n",
       "          0.98431379,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "          0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "          0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "          0.74117649,  0.09019608,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.88627458,  0.99607849,  0.81568635,\n",
       "          0.78039223,  0.78039223,  0.78039223,  0.78039223,  0.54509807,\n",
       "          0.2392157 ,  0.2392157 ,  0.2392157 ,  0.2392157 ,  0.2392157 ,\n",
       "          0.50196081,  0.8705883 ,  0.99607849,  0.99607849,  0.74117649,\n",
       "          0.08235294,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.14901961,  0.32156864,  0.0509804 ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.13333334,\n",
       "          0.83529419,  0.99607849,  0.99607849,  0.45098042,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.32941177,  0.99607849,\n",
       "          0.99607849,  0.91764712,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.32941177,  0.99607849,  0.99607849,  0.91764712,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.41568631,  0.6156863 ,\n",
       "          0.99607849,  0.99607849,  0.95294124,  0.20000002,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.09803922,  0.45882356,  0.89411771,  0.89411771,\n",
       "          0.89411771,  0.99215692,  0.99607849,  0.99607849,  0.99607849,\n",
       "          0.99607849,  0.94117653,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.26666668,  0.4666667 ,  0.86274517,\n",
       "          0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "          0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.55686277,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.14509805,  0.73333335,\n",
       "          0.99215692,  0.99607849,  0.99607849,  0.99607849,  0.87450987,\n",
       "          0.80784321,  0.80784321,  0.29411766,  0.26666668,  0.84313732,\n",
       "          0.99607849,  0.99607849,  0.45882356,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.44313729,  0.8588236 ,  0.99607849,  0.94901967,  0.89019614,\n",
       "          0.45098042,  0.34901962,  0.12156864,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.7843138 ,  0.99607849,  0.9450981 ,\n",
       "          0.16078432,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.66274512,  0.99607849,\n",
       "          0.6901961 ,  0.24313727,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.18823531,\n",
       "          0.90588242,  0.99607849,  0.91764712,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.07058824,  0.48627454,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.32941177,  0.99607849,  0.99607849,\n",
       "          0.65098041,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.54509807,  0.99607849,  0.9333334 ,  0.22352943,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.82352948,  0.98039222,  0.99607849,\n",
       "          0.65882355,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.94901967,  0.99607849,  0.93725497,  0.22352943,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.34901962,  0.98431379,  0.9450981 ,\n",
       "          0.33725491,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.01960784,\n",
       "          0.80784321,  0.96470594,  0.6156863 ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.01568628,  0.45882356,  0.27058825,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ]], dtype=float32),\n",
       " array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.next_batch(1)\n",
    "# data looks like [all_the_data, all_the_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for _ in  range(1000):\n",
    "    batch = mnist.train.next_batch(100)\n",
    "    train_step.run(feed_dict = {x: batch[0], y_: batch[1]} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9129\n"
     ]
    }
   ],
   "source": [
    "# Creates a list of booleans\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(accuracy.eval(feed_dict = {x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Convenience functions for initializing weights to non-zeros \n",
    "\n",
    "def weight_variable(shape):\n",
    "    # truncated normal is just the normal distribution with the ends bounded (relative to stddev hopefully)\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# Positive bias because we'll be using ReLu's\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(.1, shape = shape)\n",
    "    return tf.Variable(initial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def conv2d(x, W):\n",
    "    # with stride length 1 and zero padding, the output of this is the same shape as x\n",
    "    return tf.nn.conv2d(x, W, strides = [1, 1, 1, 1], padding = \"SAME\")\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    # ksize corresponds to the size of the window and strides is how we move that window\n",
    "    # ksize is [1, 2, 2, 1] because we work on one image at a time, look at pixels 2x2, and there is only 1 channel\n",
    "    return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \"SAME\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 1st convolution layer\n",
    "W_conv1 = weight_variable([5, 5, 1, 32]) # 5x5 window, 1 input channel, 32 windows in total\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1]) # Unknown number of images, 28x28 images, 1 channel (greyscale)\n",
    "\n",
    "# What we're doing here is\n",
    "# image => convolve it => add biases => relu it => max_pool it\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 2nd convolution layer\n",
    "W_conv2 = weight_variable([5, 5, 32, 64]) # 5x5x32 windows, 64 windows\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "# 14x14x32 space => 7x7x64 space\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 7x7x64 space => 1024 space\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64]) # flatten the n x n from the pool to a n^2 x 1 array\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Add dropout to prevent overfitting\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Finally map to 10 dimensional space\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Error term\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = y_conv))\n",
    "# Some better backprop optimizer..\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Calculating accuracy\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, accuracy on training data: 0.08\n",
      "step: 10, accuracy on training data: 0.16\n",
      "step: 20, accuracy on training data: 0.44\n",
      "step: 30, accuracy on training data: 0.44\n",
      "step: 40, accuracy on training data: 0.64\n",
      "step: 50, accuracy on training data: 0.66\n",
      "step: 60, accuracy on training data: 0.82\n",
      "step: 70, accuracy on training data: 0.76\n",
      "step: 80, accuracy on training data: 0.88\n",
      "step: 90, accuracy on training data: 0.78\n",
      "step: 100, accuracy on training data: 0.86\n",
      "step: 110, accuracy on training data: 0.86\n",
      "step: 120, accuracy on training data: 0.86\n",
      "step: 130, accuracy on training data: 0.94\n",
      "step: 140, accuracy on training data: 0.88\n",
      "step: 150, accuracy on training data: 0.88\n",
      "step: 160, accuracy on training data: 0.96\n",
      "step: 170, accuracy on training data: 0.88\n",
      "step: 180, accuracy on training data: 0.92\n",
      "step: 190, accuracy on training data: 0.94\n",
      "step: 200, accuracy on training data: 0.88\n",
      "step: 210, accuracy on training data: 0.88\n",
      "step: 220, accuracy on training data: 0.84\n",
      "step: 230, accuracy on training data: 0.9\n",
      "step: 240, accuracy on training data: 0.92\n",
      "step: 250, accuracy on training data: 0.88\n",
      "step: 260, accuracy on training data: 0.92\n",
      "step: 270, accuracy on training data: 0.9\n",
      "step: 280, accuracy on training data: 0.92\n",
      "step: 290, accuracy on training data: 0.96\n",
      "step: 300, accuracy on training data: 0.9\n",
      "step: 310, accuracy on training data: 0.86\n",
      "step: 320, accuracy on training data: 0.94\n",
      "step: 330, accuracy on training data: 0.94\n",
      "step: 340, accuracy on training data: 0.92\n",
      "step: 350, accuracy on training data: 0.92\n",
      "step: 360, accuracy on training data: 0.96\n",
      "step: 370, accuracy on training data: 0.96\n",
      "step: 380, accuracy on training data: 0.94\n",
      "step: 390, accuracy on training data: 0.86\n",
      "step: 400, accuracy on training data: 0.88\n",
      "step: 410, accuracy on training data: 0.92\n",
      "step: 420, accuracy on training data: 0.94\n",
      "step: 430, accuracy on training data: 0.98\n",
      "step: 440, accuracy on training data: 0.92\n",
      "step: 450, accuracy on training data: 0.94\n",
      "step: 460, accuracy on training data: 0.86\n",
      "step: 470, accuracy on training data: 0.96\n",
      "step: 480, accuracy on training data: 0.84\n",
      "step: 490, accuracy on training data: 0.94\n",
      "step: 500, accuracy on training data: 0.84\n",
      "step: 510, accuracy on training data: 0.94\n",
      "step: 520, accuracy on training data: 0.92\n",
      "step: 530, accuracy on training data: 0.94\n",
      "step: 540, accuracy on training data: 0.94\n",
      "step: 550, accuracy on training data: 0.96\n",
      "step: 560, accuracy on training data: 0.98\n",
      "step: 570, accuracy on training data: 0.94\n",
      "step: 580, accuracy on training data: 0.9\n",
      "step: 590, accuracy on training data: 0.9\n",
      "step: 600, accuracy on training data: 1\n",
      "step: 610, accuracy on training data: 0.92\n",
      "step: 620, accuracy on training data: 0.98\n",
      "step: 630, accuracy on training data: 0.94\n",
      "step: 640, accuracy on training data: 0.96\n",
      "step: 650, accuracy on training data: 0.98\n",
      "step: 660, accuracy on training data: 0.92\n",
      "step: 670, accuracy on training data: 1\n",
      "step: 680, accuracy on training data: 0.96\n",
      "step: 690, accuracy on training data: 0.94\n",
      "step: 700, accuracy on training data: 1\n",
      "step: 710, accuracy on training data: 0.88\n",
      "step: 720, accuracy on training data: 0.92\n",
      "step: 730, accuracy on training data: 0.96\n",
      "step: 740, accuracy on training data: 0.96\n",
      "step: 750, accuracy on training data: 0.9\n",
      "step: 760, accuracy on training data: 0.96\n",
      "step: 770, accuracy on training data: 0.92\n",
      "step: 780, accuracy on training data: 0.92\n",
      "step: 790, accuracy on training data: 0.9\n",
      "step: 800, accuracy on training data: 0.98\n",
      "step: 810, accuracy on training data: 0.96\n",
      "step: 820, accuracy on training data: 0.96\n",
      "step: 830, accuracy on training data: 0.92\n",
      "step: 840, accuracy on training data: 1\n",
      "step: 850, accuracy on training data: 0.94\n",
      "step: 860, accuracy on training data: 0.96\n",
      "step: 870, accuracy on training data: 0.92\n",
      "step: 880, accuracy on training data: 0.92\n",
      "step: 890, accuracy on training data: 0.98\n",
      "step: 900, accuracy on training data: 1\n",
      "step: 910, accuracy on training data: 0.96\n",
      "step: 920, accuracy on training data: 0.96\n",
      "step: 930, accuracy on training data: 0.94\n",
      "step: 940, accuracy on training data: 0.96\n",
      "step: 950, accuracy on training data: 0.98\n",
      "step: 960, accuracy on training data: 0.94\n",
      "step: 970, accuracy on training data: 0.94\n",
      "step: 980, accuracy on training data: 1\n",
      "step: 990, accuracy on training data: 0.94\n"
     ]
    }
   ],
   "source": [
    "# Train the thing!\n",
    "\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(1000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    train_step.run(feed_dict = {x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "    if i % 10 == 0:\n",
    "        # don't drop out when evaluating the model\n",
    "        train_accuracy = accuracy.eval({x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print (\"step: %d, accuracy on training data: %g\" % (i, train_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# accuracy.eval(feed_dict = {x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<tensorflow.python.ops.variables.Variable object at 0x1163e9908>, <tensorflow.python.ops.variables.Variable object at 0x1163e9e80>, <tensorflow.python.ops.variables.Variable object at 0x1163f3f98>, <tensorflow.python.ops.variables.Variable object at 0x1162d6940>, <tensorflow.python.ops.variables.Variable object at 0x116320668>}\n",
      "[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0\n",
      "[[  442.36846924  -538.46972656  -123.18572998   802.9821167   -148.46865845\n",
      "   -125.98188019  -227.18161011   -42.62307358  1141.20605469\n",
      "   -359.49813843]]\n",
      "1679.68\n",
      "50\n",
      "[[ 384.02713013 -171.60749817  -87.23052216  791.39782715 -150.56288147\n",
      "  -157.84642029 -178.1633606   -50.54122543  787.47601318 -283.22457886]]\n",
      "963.025\n",
      "100\n",
      "[[ 337.56201172  199.34260559  -78.91117859  606.19915771 -134.13731384\n",
      "  -159.78501892 -133.99649048  -72.19837189  606.44195557 -213.56018066]]\n",
      "407.678\n",
      "150\n",
      "[[ 306.32843018  510.35140991  -54.85453796  453.15570068 -132.23455811\n",
      "  -184.17518616  -61.36701202  -76.83322906  452.66677856 -162.74649048]]\n",
      "0.0\n",
      "200\n",
      "[[ 303.15298462  532.74133301  -51.57759857  440.93023682 -131.37063599\n",
      "  -185.58532715  -55.40869904  -77.25347137  440.14450073 -159.22660828]]\n",
      "0.0\n",
      "250\n",
      "[[ 303.13504028  532.87194824  -51.55802536  440.85882568 -131.36553955\n",
      "  -185.59230042  -55.37319183  -77.25691223  440.07131958 -159.20535278]]\n",
      "0.0\n",
      "300\n",
      "[[ 303.13485718  532.8717041   -51.55802917  440.85861206 -131.36547852\n",
      "  -185.5921936   -55.37296295  -77.25681305  440.07119751 -159.20521545]]\n",
      "0.0\n",
      "350\n",
      "[[ 303.13485718  532.8717041   -51.55802917  440.85861206 -131.36547852\n",
      "  -185.5921936   -55.37296295  -77.25681305  440.07119751 -159.20521545]]\n",
      "0.0\n",
      "400\n",
      "[[ 303.13485718  532.8717041   -51.55802917  440.85861206 -131.36547852\n",
      "  -185.5921936   -55.37296295  -77.25681305  440.07119751 -159.20521545]]\n",
      "0.0\n",
      "450\n",
      "[[ 303.13485718  532.8717041   -51.55802917  440.85861206 -131.36547852\n",
      "  -185.5921936   -55.37296295  -77.25681305  440.07119751 -159.20521545]]\n",
      "0.0\n",
      "500\n",
      "[[ 303.13485718  532.8717041   -51.55802917  440.85861206 -131.36547852\n",
      "  -185.5921936   -55.37296295  -77.25681305  440.07119751 -159.20521545]]\n",
      "0.0\n",
      "550\n",
      "[[ 303.13485718  532.8717041   -51.55802917  440.85861206 -131.36547852\n",
      "  -185.5921936   -55.37296295  -77.25681305  440.07119751 -159.20521545]]\n",
      "0.0\n",
      "600\n",
      "[[ 303.13485718  532.8717041   -51.55802917  440.85861206 -131.36547852\n",
      "  -185.5921936   -55.37296295  -77.25681305  440.07119751 -159.20521545]]\n",
      "0.0\n",
      "650\n",
      "[[ 303.13485718  532.8717041   -51.55802917  440.85861206 -131.36547852\n",
      "  -185.5921936   -55.37296295  -77.25681305  440.07119751 -159.20521545]]\n",
      "0.0\n",
      "700\n",
      "[[ 303.13485718  532.8717041   -51.55802917  440.85861206 -131.36547852\n",
      "  -185.5921936   -55.37296295  -77.25681305  440.07119751 -159.20521545]]\n",
      "0.0\n",
      "750\n",
      "[[ 303.13485718  532.8717041   -51.55802917  440.85861206 -131.36547852\n",
      "  -185.5921936   -55.37296295  -77.25681305  440.07119751 -159.20521545]]\n",
      "0.0\n",
      "800\n",
      "[[ 303.13485718  532.8717041   -51.55802917  440.85861206 -131.36547852\n",
      "  -185.5921936   -55.37296295  -77.25681305  440.07119751 -159.20521545]]\n",
      "0.0\n",
      "850\n",
      "[[ 303.13485718  532.8717041   -51.55802917  440.85861206 -131.36547852\n",
      "  -185.5921936   -55.37296295  -77.25681305  440.07119751 -159.20521545]]\n",
      "0.0\n",
      "900\n",
      "[[ 303.13485718  532.8717041   -51.55802917  440.85861206 -131.36547852\n",
      "  -185.5921936   -55.37296295  -77.25681305  440.07119751 -159.20521545]]\n",
      "0.0\n",
      "950\n",
      "[[ 303.13485718  532.8717041   -51.55802917  440.85861206 -131.36547852\n",
      "  -185.5921936   -55.37296295  -77.25681305  440.07119751 -159.20521545]]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prior_all_vars = tf.global_variables()\n",
    "\n",
    "# What if we try generating images by training on the input itself!\n",
    "random_image = tf.Variable(tf.random_uniform([1, 28, 28, 1], minval = 0, maxval = 255))\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(random_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64]) # flatten the n x n from the pool to a n^2 x 1 array\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "y_conv2 = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = y_conv2))\n",
    "train_step2 = tf.train.AdamOptimizer(1e-1).minimize(cross_entropy, var_list = [random_image])\n",
    "\n",
    "# uninitialized_vars = []\n",
    "# for var in tf.global_variables():\n",
    "#     print(tf.is_variable_initialized(var).eval())\n",
    "# #     if (tf.is_variable_initialized(var) is None):\n",
    "# #         uninitialized_vars.append(var)\n",
    "# print(uninitialized_vars)\n",
    "# sesson.run(tf.vari)\n",
    "        \n",
    "now_all_vars = tf.global_variables()\n",
    "    \n",
    "vars_to_init = set(now_all_vars) - set(prior_all_vars)\n",
    "print(vars_to_init)\n",
    "session.run(tf.variables_initializer(vars_to_init))\n",
    "    \n",
    "import numpy as np\n",
    "\n",
    "one_shot = np.zeros([10])\n",
    "one_shot[1] = 1\n",
    "print(one_shot)\n",
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib.pyplot import plot\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "import PIL.Image\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "#     if i % 1 == 0:\n",
    "#         print(i)\n",
    "#         print(cross_entropy.eval(feed_dict = {y_: [one_shot], keep_prob: 1.0}))\n",
    "    if i % 50 == 0:\n",
    "        print(i)\n",
    "        pixels = random_image.eval()\n",
    "        pixels = np.reshape(pixels, [28, 28])\n",
    "#         PIL.Image.fromarray(pixels).show()\n",
    "        print(y_conv2.eval(feed_dict = {keep_prob: 1.0}))\n",
    "        print(cross_entropy.eval(feed_dict = {y_: [one_shot], keep_prob: 1.0}))\n",
    "        \n",
    "    train_step2.run(feed_dict = {y_: [one_shot], keep_prob: 0.5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "random_image_2d = np.reshape(random_image.eval(), [28, 28])\n",
    "random_image_1d = np.reshape(random_image_2d, [28 * 28])\n",
    "\n",
    "y_conv.eval(feed_dict={x: [random_image_1d], keep_prob: 1.0})\n",
    "\n",
    "PIL.Image.fromarray(random_image_2d).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
